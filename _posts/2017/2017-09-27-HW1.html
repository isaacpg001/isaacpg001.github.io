
---
layout: post
title: iOS VoiceOver Programming Guide
categories:
- Programming
tags:
- HomeWorks
---




Q1.

<img src="https://image.ibb.co/noVg65/1505884346.jpg" width = "300" height = "200" alt="图片名称" align=center />

The sample space is drawed above, colored blue, from which we could derive the marginal PDF of x as 

$$f_X(x) = \int_x^{x+1} dy = 1, \ \ \ x\in (0,1);$$ $$f_Y(y) = \left\{ \begin{array}{cl} \int_0^ydx = y,&0<y<=1\\\int_{y-1}^1dx = 2-y,&1<y<2\end{array}\right.$$


Then $$u_X = E_X[X] = \int_0^1 x f_X(x) dx = 1/2;\ \ u_y = E_Y[Y] = \int_0^1 y f_Y(y) dy  +\int_1^2 y f_Y(y) dy = 1/3 + 2/3 = 1$$

$$ E[XY] = \int_0^1dx[\int_x^{x+1} xy dy] = \int_0^1 x(x+.5)dx = 7/12$$
Therefore, the covariance of $X$ and $Y$ could be written as 

$$Cov(X,Y) = E[XY]- E[X]E[Y] = 1/12$$

Q2.

(a)

$$P(Z=z,W=1) = \int_0 ^z \lambda e^{-\lambda x}dx[\int_x^\infty \mu e^{-\mu y} dy = \lambda \int_0^z e^{-(\lambda+\mu)x}dx$$

Therefore

$$ f(z,W=1) = \partial_z P(Z=z,W=1) = \lambda e^{-(\lambda+\mu)z}$$

Similarly,

$$f(Z=z,W=0)  =\mu e^{-(\lambda+\mu)z}$$

The joint probability density distribution is 

$$f(z,w) = \lambda e^{-(\lambda+\mu)z} \mathcal{I}_{w=1} + \mu e^{-(\lambda+\mu)z} \mathcal{I}_{w=0}$$

(b)

Marginal distribution of W:
$$P(w=1) = \int_0^\infty f(z,W=1) dz = \frac{\lambda}{\lambda +\mu}$$
$$P(w=0) = \int_0^\infty f(z,W=0) dz = \frac{\mu}{\lambda +\mu}$$

Marginal distribution of Z:
$$ f_Z(z)= \sum_w f(z,w) = (\lambda+\mu) e^{-(\lambda+\mu)z}$$


(c) 

$$ f_z(z) P(w=1)  = (\lambda+\mu) e^{-(\lambda+\mu)z}  * \frac{\lambda}{\lambda +\mu} = \lambda  e^{-(\lambda+\mu)z} = f(Z=z,W=1)$$
$$ f_z(z) P(w=0)  = (\lambda+\mu) e^{-(\lambda+\mu)z}  * \frac{\mu}{\lambda +\mu} = \mu  e^{-(\lambda+\mu)z} = f(Z=z,W=0)$$

which tells $f(z,w) = f_Z(z) P_W(w)$ holds for arbitary w, and z. Then we could conclude that $Z$ and $W$ are independent.

Q3.

(a)

$$E[X_i] = E_{P_i} \{E_{X_i} [X_i|P_i]\} =E_{P_i} [P_i] = \frac{\alpha}{\alpha+\beta} $$

As different $X$ are independent, we could have 

$$E[Y] = E[\sum_i X_i] = \frac{n\alpha}{\alpha+\beta}$$

(b）

$$E[X_i^2 ] =  E_{P_i} \{E_{X_i} [X_i^2|P_i]\} =E_{P_i} [P_i] = \frac{\alpha}{\alpha+\beta} $$

$$ E[Y^2 ] =E[(\sum_i X_i)^2 ] = \sum_i E[X_i^2] + \sum_{i\neq j}E[X_i]E[X_j] =  \frac{n \alpha}{\alpha+\beta} + \frac{n(n-1)\alpha^2}{(\alpha+\beta)^2}$$

Then 

$$Var(Y) = E[Y^2]-(E[Y])^2 = \frac{n\alpha\beta}{(\alpha+\beta)^2}$$

hence Y has the same mean and variance as a binomial$(n,\alpha/(\alpha+\beta))$.


$$P_X(x) = \int_{-\infty}^\infty P(x|y) f_Y(y)dy$$

Then we could have 

$$ P_X(x_i) = \left\{ \begin{array}{cl}\int_{-\infty}^{\infty} p_i f(p_i) dp_i = \frac{\alpha}{\alpha+\beta},& x_i = 1\\\frac{\beta}{\alpha+\beta},&x_i =0\end{array}\right.$$

Since each $X_i$ would be independent, $Y=\sum_i X_i$ should be the binominal distribution as binomial$(n,\alpha/(\alpha+\beta))$.

(c)

$$E[X_i] = E_{P_i} \{E_{X_i} [X_i|P_i]\} = n_i E_{P_i} [P_i] = \frac{\alpha n_i}{\alpha+\beta} $$

$$
\begin{array}{cl}
E[X_i^2] &= E_{P_i} \{E [X_i^2|P_i]\} \\
&= E_{P_i} \{Var(X_i|P_i)+E^2[X_i|P_i]\}\\
&= n_i E_{P_i} \{P_i\}+(n_i^2-n_i) E_{P_i} \{P_i^2\} \\
&= \frac{n_i \alpha}{\alpha+\beta} + (n_i^2-n_i) \frac{\alpha(\alpha+1)}{(\alpha+\beta)(\alpha+\beta+1)}\\
&= \frac{n_i\alpha(n_i\alpha + \beta+n_i)}{(\alpha+\beta)(\alpha+\beta+1)}
\end{array}
$$

$$Var[X_i] = E[X_i^2] - E^2(X_i) = \frac{n_i\alpha\beta(\alpha+\beta+n_i)}{(\alpha+\beta)^2(\alpha+\beta+1)}$$


Since different variables are independent, we could have 

$$ Var[Y] =\sum_i Var[X_i] = \sum_i \frac{n_i\alpha\beta(\alpha+\beta+n_i)}{(\alpha+\beta)^2(\alpha+\beta+1)}$$

4.

(a)
$$f(u = x+y,v=x-y) = \rho_{XY} (x=(u+v)/2,y = (u-v)/2) |\frac{dxdy}{dudv}|= \frac{1}{4\pi}  e^{-\frac{(u+v)^2 +(u-v)^2}{8}}= \frac{1}{4\pi}  e^{-\frac{u^2 +v^2}{4}}$$

(b)  

The margianl distribution of X+Y,denoted as U is calculated as 

$$f_U(u)=\int_{-\infty}^{\infty} f(u,v)dv = \frac{1}{2\sqrt{\pi}}  e^{-\frac{u^2 }{2}}$$

Also $v= X-Y$,

$$f_V(v)= \frac{1}{2\sqrt{\pi}}  e^{-\frac{v^2 }{2}}$$

From the calculations above, we could verify that $f(u,v) = f_U(u)f_V(v)$ holds for arbitary u,v, indicating they are independent and uncorrelated.

(c) Bivariate normal distribution have such a form 

$$ f(\mathbf{x})\propto e^{ - \frac{\mathbf{x }^T \sum^{-1} \mathbf{x}}{2}}$$.

whose covariance matrix is  $ \sum$. Uncorrelation implies $\sum$ is totally diagonal. Then we the distribution could always be denoted as the product of two independent normal distribution.  Below is the general form of biviriate normal distribution. 

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/c6fc534bfde62d6d2b3b743b0c3fa2fb7fc3174a)



Q5.

(1). 

From the statements, we always have $\partial_y x |=1$, then

$$ f_Y(y) = \left\{ 
\begin{array}{cl}
f_X(y), & |y|\ge a\\
f_X(-y), & |y|<a \end{array}\right. $$

Since $f_X$ are symmetric, we have that $f_Y(y) = f_X(y)$ is a standard normal r.v.

(2)
$$ E[XY] =E_X \big\{ E[XY|X] \big\} = E_X \big \{ X^2 \times sign(X^2-a^2) \big \}= 2 \int_a^\infty x^2 f(x) dx -2 \int_{0}^a x ^2 f(x) dx  =0$$

Therefore they are uncorrelated.

(3)

Independency implies that $f(y|X=x_1) = f(y|X=x_2)$, for arbitary $x_1$ and $x_2$. Since $f(y|X=a) = \delta(y-a)$, $f(y|X=-a) = \delta(y+a)$, we have $ f(y|X=a) \neq f(y|X=-a)$, which indicates that X and Y are not independent.

(4) 

For bivirate distribution, the conditional distribution of one to the other is also normal distribution. Here distribution of $y$ with fixed $x$ is not normal. Hence, the joint distribution of $x$ and $y$ is not bivirate normal, and the result above will not be contradictory to Q4.

Q6.

```R
x = rexp(1000,rate = 1);
y = rnorm(1000,mean = 0, sd =x);
z = rexp(1000, rate = x);
cor(y,z)
```

Result: 0.0006230831
